{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11797813,"sourceType":"datasetVersion","datasetId":7408624}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPt+/7KwOWko/9wU0LdJ48G"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\nfrom transformers import EarlyStoppingCallback","metadata":{"id":"zY0Klne4XBIX","executionInfo":{"status":"ok","timestamp":1746788000586,"user_tz":-180,"elapsed":19217,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:49:15.998111Z","iopub.execute_input":"2025-05-13T17:49:15.998812Z","iopub.status.idle":"2025-05-13T17:49:24.795606Z","shell.execute_reply.started":"2025-05-13T17:49:15.998785Z","shell.execute_reply":"2025-05-13T17:49:24.795038Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"2025-05-13 17:49:22.030802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747158562.053174     303 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747158562.060293     303 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install trl==0.11.0\n! pip install -U bitsandbytes","metadata":{"id":"V531RX7F156n","executionInfo":{"status":"ok","timestamp":1746787830741,"user_tz":-180,"elapsed":130056,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"outputId":"41054586-66f1-426a-b677-6502be5fbc37","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## upload dataset","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/bilingual-construction-dataset/bilingual_construction_dataset_translated.json', 'r') as file:\n    data = json.load(file)\n","metadata":{"id":"eiNuy0ZhY96-","executionInfo":{"status":"ok","timestamp":1746779380905,"user_tz":-180,"elapsed":117,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:50:36.408677Z","iopub.execute_input":"2025-05-13T17:50:36.409414Z","iopub.status.idle":"2025-05-13T17:50:36.489644Z","shell.execute_reply.started":"2025-05-13T17:50:36.409387Z","shell.execute_reply":"2025-05-13T17:50:36.489102Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### format the json file","metadata":{}},{"cell_type":"code","source":"formatted_bilingual = [\n    {\"text\": f\"### Human: {row['instruction_en']}\\n### Assistant: {row['response_en']}\"} \n    for row in data\n] + [\n    {\"text\": f\"### Human: {row['instruction_ar']}\\n### Assistant: {row['response_ar']}\"} \n    for row in data\n]\n\ndataset = Dataset.from_pandas(pd.DataFrame(formatted_bilingual))\ndataset = dataset.train_test_split(test_size=0.1)","metadata":{"id":"DSQjNcyZncw2","executionInfo":{"status":"ok","timestamp":1746779384622,"user_tz":-180,"elapsed":815,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:50:46.417110Z","iopub.execute_input":"2025-05-13T17:50:46.417396Z","iopub.status.idle":"2025-05-13T17:50:46.497344Z","shell.execute_reply.started":"2025-05-13T17:50:46.417375Z","shell.execute_reply":"2025-05-13T17:50:46.496585Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:17:50.645860Z","iopub.execute_input":"2025-05-13T22:17:50.646217Z","iopub.status.idle":"2025-05-13T22:17:50.662565Z","shell.execute_reply.started":"2025-05-13T22:17:50.646193Z","shell.execute_reply":"2025-05-13T22:17:50.661752Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3200aa0d17894d968ee4ac293da6f7b5"}},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"## upload the tokenizer and the model","metadata":{}},{"cell_type":"code","source":"# --- Tokenizer ---\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"1fkxgUKlnrFN","executionInfo":{"status":"ok","timestamp":1746779389993,"user_tz":-180,"elapsed":365,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:50:57.122561Z","iopub.execute_input":"2025-05-13T17:50:57.122834Z","iopub.status.idle":"2025-05-13T17:50:57.397391Z","shell.execute_reply.started":"2025-05-13T17:50:57.122814Z","shell.execute_reply":"2025-05-13T17:50:57.396620Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)","metadata":{"id":"qtiM_8vDo_-a","executionInfo":{"status":"error","timestamp":1746788093111,"user_tz":-180,"elapsed":988,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"outputId":"c72f7202-bb32-47e3-f92e-29dc3655e1bc","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:51:04.929745Z","iopub.execute_input":"2025-05-13T17:51:04.930480Z","iopub.status.idle":"2025-05-13T17:51:25.849381Z","shell.execute_reply.started":"2025-05-13T17:51:04.930452Z","shell.execute_reply":"2025-05-13T17:51:25.848708Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a334b1d502db4d68af12f5d44c81a097"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"id":"PZTFUVEAHCw6","executionInfo":{"status":"ok","timestamp":1746778568636,"user_tz":-180,"elapsed":97294,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"outputId":"b6b51508-8855-42be-8eea-98aa2f58e849","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### use lora configration","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"id":"-w1gzIUerDse","executionInfo":{"status":"ok","timestamp":1746779396816,"user_tz":-180,"elapsed":73,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:51:33.307279Z","iopub.execute_input":"2025-05-13T17:51:33.307892Z","iopub.status.idle":"2025-05-13T17:51:33.327860Z","shell.execute_reply.started":"2025-05-13T17:51:33.307866Z","shell.execute_reply":"2025-05-13T17:51:33.327343Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"id":"ApYHgADUrKWW","executionInfo":{"status":"ok","timestamp":1746779404531,"user_tz":-180,"elapsed":268,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"outputId":"df1af85c-f426-42ff-f2a0-b481975ee678","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:51:39.433532Z","iopub.execute_input":"2025-05-13T17:51:39.434197Z","iopub.status.idle":"2025-05-13T17:51:39.670384Z","shell.execute_reply.started":"2025-05-13T17:51:39.434168Z","shell.execute_reply":"2025-05-13T17:51:39.669783Z"}},"outputs":[{"name":"stdout","text":"trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### setting training parameters","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./llama2-7b-egyptian-construction-lora\",\n    per_device_train_batch_size=1,      # Lowered\n    per_device_eval_batch_size=1,       # Lowered\n    gradient_accumulation_steps=2,      # Or 1 if still OOM\n    max_steps=1000,                      # For quick test\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=50,\n    save_steps=100,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    run_name=\"egyptian-construction-chatbot-v1\",\n)","metadata":{"id":"nTsH47odIkb0","executionInfo":{"status":"ok","timestamp":1746779406878,"user_tz":-180,"elapsed":57,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:51:48.151374Z","iopub.execute_input":"2025-05-13T17:51:48.152090Z","iopub.status.idle":"2025-05-13T17:51:48.179643Z","shell.execute_reply.started":"2025-05-13T17:51:48.152047Z","shell.execute_reply":"2025-05-13T17:51:48.179106Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(dataset[\"train\"][0])\nprint(dataset[\"train\"].features)","metadata":{"id":"A6Yyds_4xRHE","executionInfo":{"status":"ok","timestamp":1746728494553,"user_tz":-180,"elapsed":17,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"outputId":"cb1905eb-6cb7-4ac9-cb5d-4b32198aea84","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install wandb --quiet\n!wandb login 771825ea45b66d37e930eaecd1a00b8fe61ccfed\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### intialiaze wnadb project to track the training","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(\n    project=\"egyptian-construction-chatbot-v1\",\n    name=\"debug-run\",\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trainer setup\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    args=training_args,\n    tokenizer=tokenizer,\n    peft_config=lora_config,\n    dataset_text_field=\"text\",\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=20)],\n)","metadata":{"id":"pyqnuBMLJVNk","executionInfo":{"status":"error","timestamp":1746787890487,"user_tz":-180,"elapsed":370,"user":{"displayName":"Ahmed Hussein","userId":"02317312714693905782"}},"outputId":"4ff9619b-d0f1-455e-f4cf-b23994e28054","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:03.641358Z","iopub.execute_input":"2025-05-13T17:52:03.641883Z","iopub.status.idle":"2025-05-13T17:52:06.684042Z","shell.execute_reply.started":"2025-05-13T17:52:03.641861Z","shell.execute_reply":"2025-05-13T17:52:06.683326Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10404 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ab640898894b9bb45fb96f7e3b7d3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1156 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4da55099baf4b6596340d02c2aebc20"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer.train()\nmodel.save_pretrained(\"./lora\")\ntokenizer.save_pretrained(\"./lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:13.081681Z","iopub.execute_input":"2025-05-13T17:52:13.082421Z","iopub.status.idle":"2025-05-13T21:11:25.577181Z","shell.execute_reply.started":"2025-05-13T17:52:13.082398Z","shell.execute_reply":"2025-05-13T21:11:25.576411Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmeddewdar45\u001b[0m (\u001b[33mahmeddewdar45-alexandria-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250513_175219-9ap66gkp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ahmeddewdar45-alexandria-university/huggingface/runs/9ap66gkp' target=\"_blank\">egyptian-construction-chatbot-v1</a></strong> to <a href='https://wandb.ai/ahmeddewdar45-alexandria-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ahmeddewdar45-alexandria-university/huggingface' target=\"_blank\">https://wandb.ai/ahmeddewdar45-alexandria-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ahmeddewdar45-alexandria-university/huggingface/runs/9ap66gkp' target=\"_blank\">https://wandb.ai/ahmeddewdar45-alexandria-university/huggingface/runs/9ap66gkp</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 3:18:54, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.442200</td>\n      <td>1.478307</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.370300</td>\n      <td>1.391276</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.310100</td>\n      <td>1.350127</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.279100</td>\n      <td>1.310227</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.173300</td>\n      <td>1.285482</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.205200</td>\n      <td>1.261849</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.194900</td>\n      <td>1.246149</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.159200</td>\n      <td>1.230777</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.199000</td>\n      <td>1.222252</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.145300</td>\n      <td>1.217482</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"('./lora/tokenizer_config.json',\n './lora/special_tokens_map.json',\n './lora/tokenizer.model',\n './lora/added_tokens.json',\n './lora/tokenizer.json')"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"  print(len(dataset[\"train\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### load the model to hugging face repositery","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\napi.upload_folder(\n    folder_path=\"./lora\",\n    repo_id=\"AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:18:50.634404Z","iopub.execute_input":"2025-05-13T22:18:50.635244Z","iopub.status.idle":"2025-05-13T22:18:55.191807Z","shell.execute_reply.started":"2025-05-13T22:18:50.635218Z","shell.execute_reply":"2025-05-13T22:18:55.191250Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b476a49ea34fa686c686a18bc65057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015393797b784f7ab3dc0346769278b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/33.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"640f23555f0a4c758af51ea80420bc62"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter/commit/29a71d13a7029314a7e1dc24a03566ef585b82a4', commit_message='Upload folder using huggingface_hub', commit_description='', oid='29a71d13a7029314a7e1dc24a03566ef585b82a4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi = HfApi()\napi.create_repo(\"AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter\", private=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:18:07.013187Z","iopub.execute_input":"2025-05-13T22:18:07.013469Z","iopub.status.idle":"2025-05-13T22:18:14.477830Z","shell.execute_reply.started":"2025-05-13T22:18:07.013450Z","shell.execute_reply":"2025-05-13T22:18:14.477267Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='AhmedHussein66/llama2-7b-egyptian-construction-lora-lora-adapter')"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"### evaluate the model ","metadata":{}},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"Evaluation results:\", eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:21:41.252633Z","iopub.execute_input":"2025-05-13T22:21:41.253347Z","iopub.status.idle":"2025-05-13T22:34:56.035102Z","shell.execute_reply.started":"2025-05-13T22:21:41.253324Z","shell.execute_reply":"2025-05-13T22:34:56.034480Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1156' max='1156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1156/1156 13:13]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 1.2174819707870483, 'eval_runtime': 794.7723, 'eval_samples_per_second': 1.455, 'eval_steps_per_second': 1.455, 'epoch': 0.1922337562475971}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### test the model","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntranslator_ar_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ar-en\")\ntranslator_en_to_ar = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ar\")\n\nchatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\ndef translate_and_chat(input_text):\n    input_en = translator_ar_to_en(input_text)[0]['translation_text']\n    \n    prompt = f\"### Human: {input_en}\\n### Assistant:\"\n    response = chatbot(prompt, max_new_tokens=100, temperature=0.7, top_p=0.9, repetition_penalty=1.2)\n    \n    assistant_response_en = response[0]['generated_text'].split(\"### Assistant:\")[-1].strip()\n    \n    assistant_response_ar = translator_en_to_ar(assistant_response_en)[0]['translation_text']\n    \n    return assistant_response_ar\n\narabic_question = \" Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† ØªØ±Ø®ÙŠØµ Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙˆØªØµØ±ÙŠØ­ Ø§Ù„ØªØ´ØºÙŠÙ„\"\nanswer = translate_and_chat(arabic_question)\nprint(\"ğŸ§  Assistant says in Arabic:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:41:00.485042Z","iopub.execute_input":"2025-05-13T22:41:00.485831Z","iopub.status.idle":"2025-05-13T22:41:10.505722Z","shell.execute_reply.started":"2025-05-13T22:41:00.485803Z","shell.execute_reply":"2025-05-13T22:41:10.504946Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nDevice set to use cuda:0\nDevice set to use cuda:0\nThe model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n","output_type":"stream"},{"name":"stdout","text":"ğŸ§  Assistant says in Arabic: ØªØ±Ø®ÙŠØµ Ø§Ù„Ø¨Ù†Ø§Ø¡ ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„Ø¨Ù†Ø§Ø¡ØŒ ÙÙŠ Ø­ÙŠÙ† ÙŠØ³Ù…Ø­ ØªØ±Ø®ÙŠØµ Ø§Ù„ØªØ´ØºÙŠÙ„ (Ù…Ø«Ù„Ø§Ù‹ Ù„Ù„Ù…ØµÙ†Ø¹) Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø¨Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø¹ÙŠÙ†Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø². ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ØªØ·Ù„Ø¨ Ù…Ù† Ù‚Ø¨Ù„ Ø³Ù„Ø·Ø§Øª Ù…Ø«Ù„ EMA Ø£Ùˆ MOPH. Ø¨Ø¯ÙˆÙ† ØªØ±Ø®ÙŠØµ ØªØ´ØºÙŠÙ„ÙŠØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ùƒ ÙØªØ­ Ø£Ø¨ÙˆØ§Ø¨Ùƒ! Ù„Ø°Ø§ Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù‡Ù…Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø´Ø±ÙˆØ¹ ÙÙŠ Ø£ÙŠ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯Ø› ÙˆØ¥Ù„Ø§ ÙØ¥Ù†Ùƒ ØªØ®Ø§Ø·Ø± Ø¨Ø¯ÙØ¹ ØºØ±Ø§Ù…Ø§Øª Ø£Ùˆ Ø£ÙˆØ§Ù…Ø± Ø¥ØºÙ„Ø§Ù‚ Ø¨Ø³Ø¨Ø¨ Ø§Ù†ØªÙ‡Ø§Ùƒ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø¨Ø¯ÙˆÙ†Ù‡Ø§.\n","output_type":"stream"}],"execution_count":46}]}